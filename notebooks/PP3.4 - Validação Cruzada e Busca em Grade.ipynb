{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais 2020.1 -- Projeto Prático 3.4\n",
    "\n",
    "**Disciplina**: Redes Neurais Artificiais 2020.1  \n",
    "**Professora**: Elloá B. Guedes (ebgcosta@uea.edu.br)  \n",
    "**Github**: http://github.com/elloa  \n",
    "        \n",
    "\n",
    "Levando em conta a base de dados **_Forest Cover Type_**, esta terceira parte do Projeto Prático 3 diz respeito à proposição e avaliação de múltiplas redes neurais artificiais do tipo feedforward multilayer perceptron para o problema da classificação multi-classe da cobertura florestal em uma área do Roosevelt National Forest.\n",
    "\n",
    "## Busca em Grade\n",
    "\n",
    "Uma maneira padrão de escolher os parâmetros de um modelo de Machine Learning é por meio de uma busca em grade via força bruta. O algoritmo da busca em grade é dado como segue:\n",
    "\n",
    "1. Escolha a métrica de desempenho que você deseja maximizar  \n",
    "2. Escolha o algoritmo de Machine Learning (exemplo: redes neurais artificiais). Em seguida, defina os parâmetros ou hiperparâmetros deste tipo de modelo sobre os quais você dseja otimizar (número de épocas, taxa de aprendizado, etc.) e construa um array de valores a serem testados para cada parâmetro ou hiperparâmetro.  \n",
    "3. Defina a grade de busca, a qual é dada como o produto cartesiano de cada parâmetro a ser testado. Por exemplo, para os arrays [50, 100, 1000] e [10, 15], tem-se que a grade é [(50,10), (50,15), (100,10), (100,15), (1000,10), (1000,15)].\n",
    "4. Para cada combinação de parâmetros a serem otimizados, utilize o conjunto de treinamento para realizar uma validação cruzada (holdout ou k-fold) e calcule a métrica de avaliação no conjunto de teste (ou conjuntos de teste)\n",
    "5. Escolha a combinação de parâmetros que maximizam a métrica de avaliação. Este é o modelo otimizado.\n",
    "\n",
    "Por que esta abordagem funciona? Porque a busca em grade efetua uma pesquisa extensiva sobre as possíveis combinações de valores para cada um dos parâmetros a serem ajustados. Para cada combinação, ela estima a performance do modelo em dados novos. Por fim, o modelo com melhor métrica de desempenho é escolhido. Tem-se então que este modelo é o que melhor pode vir a generalizar mediante dados nunca antes vistos.\n",
    "\n",
    "## Efetuando a Busca em Grade sobre Hiperparâmetros das Top-6 RNAs\n",
    "\n",
    "Considerando a etapa anterior do projeto prático, foram identificadas pelo menos 6 melhores Redes Neurais para o problema da classificação multi-classe da cobertura florestal no conjunto de dados selecionado. Algumas destas redes possuem atributos categóricos como variáveis preditoras, enquanto outras possuem apenas os atributos numéricos como preditores.\n",
    "\n",
    "A primeira etapa desta segunda parte do projeto consiste em trazer para este notebook estas seis arquiteturas, ressaltando:\n",
    "\n",
    "1. Número de neurônios ocultos por camada  \n",
    "2. Função de Ativação  \n",
    "3. Utilização ou não de atributos categóricos   \n",
    "4. Desempenho médio +- desvio padrão nos testes anteriores  \n",
    "5. Número de repetições que a equipe conseguiu realizar para verificar os resultados  \n",
    "\n",
    "Elabore uma busca em grade sobre estas arquiteturas que contemple variações nos hiperparâmetros a seguir, conforme documentação de [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "\n",
    "A. Solver  (Não usar o LBFGS, pois é mais adequado para datasets pequenos)  \n",
    "B. Batch Size  \n",
    "C. Learning Rate Init  \n",
    "D. Paciência (n_iter_no_change)  \n",
    "E. Épocas  \n",
    "\n",
    "Nesta busca em grande, contemple a utilização do objeto [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada k-fold\n",
    "\n",
    "Na elaboração da busca em grid, vamos avaliar os modelos propostos segundo uma estratégia de validação cruzada ainda não explorada até o momento: a validação cruzada k-fold. Segundo a mesma, o conjunto de dados é particionado em k partes: a cada iteração, separa-se uma das partes para teste e o modelo é treinado com as k-1 partes remanescentes. Valores sugestivos de k na literatura são k = 3, 5 ou 10, pois o custo computacional desta validação dos modelos é alto. A métrica de desempenho é resultante da média dos desempenhos nas k iterações. A figura a seguir ilustra a ideia desta avaliação\n",
    "\n",
    "<img src = \"https://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png\" width=600></img>\n",
    "\n",
    "Considerando a métrica de desempenho F1-Score, considere a validação cruzada 5-fold para aferir os resultados da busca em grande anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reservado para importação de bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 581012 entries, 0 to 581011\nData columns (total 55 columns):\n #   Column                              Non-Null Count   Dtype\n---  ------                              --------------   -----\n 0   Elevation                           581012 non-null  int64\n 1   Aspect                              581012 non-null  int64\n 2   Slope                               581012 non-null  int64\n 3   Horizontal_Distance_To_Hydrology    581012 non-null  int64\n 4   Vertical_Distance_To_Hydrology      581012 non-null  int64\n 5   Horizontal_Distance_To_Roadways     581012 non-null  int64\n 6   Hillshade_9am                       581012 non-null  int64\n 7   Hillshade_Noon                      581012 non-null  int64\n 8   Hillshade_3pm                       581012 non-null  int64\n 9   Horizontal_Distance_To_Fire_Points  581012 non-null  int64\n 10  Wilderness_Area1                    581012 non-null  int64\n 11  Wilderness_Area2                    581012 non-null  int64\n 12  Wilderness_Area3                    581012 non-null  int64\n 13  Wilderness_Area4                    581012 non-null  int64\n 14  Soil_Type1                          581012 non-null  int64\n 15  Soil_Type2                          581012 non-null  int64\n 16  Soil_Type3                          581012 non-null  int64\n 17  Soil_Type4                          581012 non-null  int64\n 18  Soil_Type5                          581012 non-null  int64\n 19  Soil_Type6                          581012 non-null  int64\n 20  Soil_Type7                          581012 non-null  int64\n 21  Soil_Type8                          581012 non-null  int64\n 22  Soil_Type9                          581012 non-null  int64\n 23  Soil_Type10                         581012 non-null  int64\n 24  Soil_Type11                         581012 non-null  int64\n 25  Soil_Type12                         581012 non-null  int64\n 26  Soil_Type13                         581012 non-null  int64\n 27  Soil_Type14                         581012 non-null  int64\n 28  Soil_Type15                         581012 non-null  int64\n 29  Soil_Type16                         581012 non-null  int64\n 30  Soil_Type17                         581012 non-null  int64\n 31  Soil_Type18                         581012 non-null  int64\n 32  Soil_Type19                         581012 non-null  int64\n 33  Soil_Type20                         581012 non-null  int64\n 34  Soil_Type21                         581012 non-null  int64\n 35  Soil_Type22                         581012 non-null  int64\n 36  Soil_Type23                         581012 non-null  int64\n 37  Soil_Type24                         581012 non-null  int64\n 38  Soil_Type25                         581012 non-null  int64\n 39  Soil_Type26                         581012 non-null  int64\n 40  Soil_Type27                         581012 non-null  int64\n 41  Soil_Type28                         581012 non-null  int64\n 42  Soil_Type29                         581012 non-null  int64\n 43  Soil_Type30                         581012 non-null  int64\n 44  Soil_Type31                         581012 non-null  int64\n 45  Soil_Type32                         581012 non-null  int64\n 46  Soil_Type33                         581012 non-null  int64\n 47  Soil_Type34                         581012 non-null  int64\n 48  Soil_Type35                         581012 non-null  int64\n 49  Soil_Type36                         581012 non-null  int64\n 50  Soil_Type37                         581012 non-null  int64\n 51  Soil_Type38                         581012 non-null  int64\n 52  Soil_Type39                         581012 non-null  int64\n 53  Soil_Type40                         581012 non-null  int64\n 54  Cover_Type                          581012 non-null  int64\ndtypes: int64(55)\nmemory usage: 243.8 MB\n"
     ]
    }
   ],
   "source": [
    "#abertura do dataset\n",
    "dados = pd.read_csv('../dataset/covtype.csv')\n",
    "dados.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dados.drop(\"Cover_Type\", axis = 1)\n",
    "y = dados.Cover_Type\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "#escalonamento\n",
    "X_train_std = (X_train - np.mean(X_train))/np.std(X_train)\n",
    "X_test_std = (X_test - np.mean(X_train))/np.std(X_train)"
   ]
  },
  {
   "source": [
    "top-6 RNAs serão consideradas as ultimas arquiteruras com 200 epocas e a utilização de atributos categóricos, para chegar a esses resultados forma realizados 10 repetições:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coloquei manualmente =, mas depois coloca direto dos dados que tu salvou anteriormente, ok?\n",
    "arquiteturas = [ 2, 'relu', 'adam', 200, (19, 16), 0.811646, 0.689338, 0.001989, 0.006959,\n",
    "1, 'tanh', 'adam', 200, 16, 0.788587, 0.644890, 0.003093, 0.009541,\n",
    "2, 'logistic', 'adam', 200, (10, 12), 0.783392, 0.644890, 0.003093, 0.009541,\n",
    "1, 'tanh', 'adam', 200, 20, 0.800555, 0.671570, 0.002680, 0.006171,\n",
    "2, 'relu', 'adam', 200, (7, 18), 0.764442, 0.601503, 0.003879, 0.024947,\n",
    "2, 'logistic', 'adam', 200, (15, 10), 0.801088, 0.674190, 0.002898, 0.010337,\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Camadas Função de Ativação Hiperparametro  Épocas Neurônios media acuracia  \\\n",
       "0       2               relu           adam     200  (19, 16)       0.811646   \n",
       "1       1               tanh           adam     200        16       0.788587   \n",
       "2       2           logistic           adam     200  (10, 12)       0.783392   \n",
       "3       1               tanh           adam     200        20       0.800555   \n",
       "4       2               relu           adam     200   (7, 18)       0.764442   \n",
       "5       2           logistic           adam     200  (15, 10)       0.801088   \n",
       "\n",
       "  media f1 score dp acuracia dp f1 score  \n",
       "0       0.689338    0.001989    0.006959  \n",
       "1        0.64489    0.003093    0.009541  \n",
       "2        0.64489    0.003093    0.009541  \n",
       "3        0.67157     0.00268    0.006171  \n",
       "4       0.601503    0.003879    0.024947  \n",
       "5        0.67419    0.002898    0.010337  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Camadas</th>\n      <th>Função de Ativação</th>\n      <th>Hiperparametro</th>\n      <th>Épocas</th>\n      <th>Neurônios</th>\n      <th>media acuracia</th>\n      <th>media f1 score</th>\n      <th>dp acuracia</th>\n      <th>dp f1 score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>relu</td>\n      <td>adam</td>\n      <td>200</td>\n      <td>(19, 16)</td>\n      <td>0.811646</td>\n      <td>0.689338</td>\n      <td>0.001989</td>\n      <td>0.006959</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>tanh</td>\n      <td>adam</td>\n      <td>200</td>\n      <td>16</td>\n      <td>0.788587</td>\n      <td>0.64489</td>\n      <td>0.003093</td>\n      <td>0.009541</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>logistic</td>\n      <td>adam</td>\n      <td>200</td>\n      <td>(10, 12)</td>\n      <td>0.783392</td>\n      <td>0.64489</td>\n      <td>0.003093</td>\n      <td>0.009541</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>tanh</td>\n      <td>adam</td>\n      <td>200</td>\n      <td>20</td>\n      <td>0.800555</td>\n      <td>0.67157</td>\n      <td>0.00268</td>\n      <td>0.006171</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>relu</td>\n      <td>adam</td>\n      <td>200</td>\n      <td>(7, 18)</td>\n      <td>0.764442</td>\n      <td>0.601503</td>\n      <td>0.003879</td>\n      <td>0.024947</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>logistic</td>\n      <td>adam</td>\n      <td>200</td>\n      <td>(15, 10)</td>\n      <td>0.801088</td>\n      <td>0.67419</td>\n      <td>0.002898</td>\n      <td>0.010337</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_arq = pd.DataFrame(np.array(arquiteturas).reshape(6,9), columns= [ \"Camadas\",\n",
    " \"Função de Ativação\", \n",
    " \"Hiperparametro\",\n",
    " \"Épocas\", \n",
    " \"Neurônios\",\n",
    " \"media acuracia\",\n",
    " \"media f1 score\", \n",
    " \"dp acuracia\", \n",
    " \"dp f1 score\"])\n",
    "\n",
    "df_arq[\"Épocas\"] = pd.to_numeric(df_arq[\"Épocas\"])\n",
    "\n",
    "df_arq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirmar parametros\n",
    "parametros = { 'solver' : ['sgd', 'adam'],\n",
    "                'batch_size' : ['auto'],\n",
    "                'learning_rate_init': [0.01, 0.1],\n",
    "                'n_iter_no_change' : [10, 30],\n",
    "                'max_iter' : [100, 150, 200]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Executando Arquitetura:  1\n",
      "Iteration 1, loss = 0.64328581\n",
      "Iteration 2, loss = 0.60433586\n",
      "Iteration 3, loss = 0.59844521\n",
      "Iteration 4, loss = 0.59432348\n",
      "Iteration 5, loss = 0.59255961\n",
      "Iteration 6, loss = 0.58935924\n",
      "Iteration 7, loss = 0.59294796\n",
      "Iteration 8, loss = 0.58964031\n",
      "Iteration 9, loss = 0.59106387\n",
      "Iteration 10, loss = 0.59054164\n",
      "Iteration 11, loss = 0.58628876\n",
      "Iteration 12, loss = 0.58884071\n",
      "Iteration 13, loss = 0.58909909\n",
      "Iteration 14, loss = 0.58983615\n",
      "Iteration 15, loss = 0.58694847\n",
      "Iteration 16, loss = 0.58916600\n",
      "Iteration 17, loss = 0.59173227\n",
      "Iteration 18, loss = 0.58727444\n",
      "Iteration 19, loss = 0.58914055\n",
      "Iteration 20, loss = 0.58668682\n",
      "Iteration 21, loss = 0.58747563\n",
      "Iteration 22, loss = 0.58746693\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64748530\n",
      "Iteration 2, loss = 0.59973938\n",
      "Iteration 3, loss = 0.59702998\n",
      "Iteration 4, loss = 0.59443953\n",
      "Iteration 5, loss = 0.59045200\n",
      "Iteration 6, loss = 0.58900770\n",
      "Iteration 7, loss = 0.58984240\n",
      "Iteration 8, loss = 0.58736752\n",
      "Iteration 9, loss = 0.58716557\n",
      "Iteration 10, loss = 0.59027070\n",
      "Iteration 11, loss = 0.58855451\n",
      "Iteration 12, loss = 0.58621115\n",
      "Iteration 13, loss = 0.58748588\n",
      "Iteration 14, loss = 0.58837256\n",
      "Iteration 15, loss = 0.59033540\n",
      "Iteration 16, loss = 0.58880654\n",
      "Iteration 17, loss = 0.58782818\n",
      "Iteration 18, loss = 0.58895316\n",
      "Iteration 19, loss = 0.58766249\n",
      "Iteration 20, loss = 0.58657997\n",
      "Iteration 21, loss = 0.58781226\n",
      "Iteration 22, loss = 0.59044747\n",
      "Iteration 23, loss = 0.59111234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64591484\n",
      "Iteration 2, loss = 0.60108511\n",
      "Iteration 3, loss = 0.59475633\n",
      "Iteration 4, loss = 0.59213946\n",
      "Iteration 5, loss = 0.58836742\n",
      "Iteration 6, loss = 0.58815290\n",
      "Iteration 7, loss = 0.59080300\n",
      "Iteration 8, loss = 0.58756302\n",
      "Iteration 9, loss = 0.59353575\n",
      "Iteration 10, loss = 0.59382587\n",
      "Iteration 11, loss = 0.59119155\n",
      "Iteration 12, loss = 0.59283510\n",
      "Iteration 13, loss = 0.59326916\n",
      "Iteration 14, loss = 0.59134193\n",
      "Iteration 15, loss = 0.59256843\n",
      "Iteration 16, loss = 0.58888284\n",
      "Iteration 17, loss = 0.59348353\n",
      "Iteration 18, loss = 0.59072992\n",
      "Iteration 19, loss = 0.59271458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64889417\n",
      "Iteration 2, loss = 0.60524318\n",
      "Iteration 3, loss = 0.59917157\n",
      "Iteration 4, loss = 0.59533717\n",
      "Iteration 5, loss = 0.59463408\n",
      "Iteration 6, loss = 0.59613810\n",
      "Iteration 7, loss = 0.59696162\n",
      "Iteration 8, loss = 0.59465466\n",
      "Iteration 9, loss = 0.59114014\n",
      "Iteration 10, loss = 0.59369220\n",
      "Iteration 11, loss = 0.59301293\n",
      "Iteration 12, loss = 0.59406774\n",
      "Iteration 13, loss = 0.59217871\n",
      "Iteration 14, loss = 0.59151180\n",
      "Iteration 15, loss = 0.59220556\n",
      "Iteration 16, loss = 0.59262744\n",
      "Iteration 17, loss = 0.59370167\n",
      "Iteration 18, loss = 0.58832030\n",
      "Iteration 19, loss = 0.59480461\n",
      "Iteration 20, loss = 0.59634011\n",
      "Iteration 21, loss = 0.59579700\n",
      "Iteration 22, loss = 0.59434077\n",
      "Iteration 23, loss = 0.59443177\n",
      "Iteration 24, loss = 0.59383847\n",
      "Iteration 25, loss = 0.59363776\n",
      "Iteration 26, loss = 0.59284658\n",
      "Iteration 27, loss = 0.59362755\n",
      "Iteration 28, loss = 0.59338492\n",
      "Iteration 29, loss = 0.59257165\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65328089\n",
      "Iteration 2, loss = 0.60923539\n",
      "Iteration 3, loss = 0.60283273\n",
      "Iteration 4, loss = 0.60119965\n",
      "Iteration 5, loss = 0.60380469\n",
      "Iteration 6, loss = 0.60211903\n",
      "Iteration 7, loss = 0.59696717\n",
      "Iteration 8, loss = 0.59675197\n",
      "Iteration 9, loss = 0.59832128\n",
      "Iteration 10, loss = 0.59793400\n",
      "Iteration 11, loss = 0.59940530\n",
      "Iteration 12, loss = 0.59701297\n",
      "Iteration 13, loss = 0.59587699\n",
      "Iteration 14, loss = 0.59798080\n",
      "Iteration 15, loss = 0.59715595\n",
      "Iteration 16, loss = 0.59361013\n",
      "Iteration 17, loss = 0.59314797\n",
      "Iteration 18, loss = 0.59499349\n",
      "Iteration 19, loss = 0.59249319\n",
      "Iteration 20, loss = 0.59317971\n",
      "Iteration 21, loss = 0.59301037\n",
      "Iteration 22, loss = 0.59455124\n",
      "Iteration 23, loss = 0.59652843\n",
      "Iteration 24, loss = 0.59385026\n",
      "Iteration 25, loss = 0.59226513\n",
      "Iteration 26, loss = 0.58974140\n",
      "Iteration 27, loss = 0.59235240\n",
      "Iteration 28, loss = 0.59422870\n",
      "Iteration 29, loss = 0.59406177\n",
      "Iteration 30, loss = 0.59370148\n",
      "Iteration 31, loss = 0.59113829\n",
      "Iteration 32, loss = 0.59284099\n",
      "Iteration 33, loss = 0.59104409\n",
      "Iteration 34, loss = 0.59038621\n",
      "Iteration 35, loss = 0.59169530\n",
      "Iteration 36, loss = 0.59319293\n",
      "Iteration 37, loss = 0.59162072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63731854\n",
      "Iteration 2, loss = 0.59872651\n",
      "Iteration 3, loss = 0.59323846\n",
      "Iteration 4, loss = 0.58815144\n",
      "Iteration 5, loss = 0.58934168\n",
      "Iteration 6, loss = 0.59085505\n",
      "Iteration 7, loss = 0.59159093\n",
      "Iteration 8, loss = 0.58828705\n",
      "Iteration 9, loss = 0.58954268\n",
      "Iteration 10, loss = 0.58831638\n",
      "Iteration 11, loss = 0.58854000\n",
      "Iteration 12, loss = 0.58912150\n",
      "Iteration 13, loss = 0.59049245\n",
      "Iteration 14, loss = 0.58989827\n",
      "Iteration 15, loss = 0.58770804\n",
      "Iteration 16, loss = 0.58473878\n",
      "Iteration 17, loss = 0.58605767\n",
      "Iteration 18, loss = 0.58613094\n",
      "Iteration 19, loss = 0.58555009\n",
      "Iteration 20, loss = 0.58942623\n",
      "Iteration 21, loss = 0.58650664\n",
      "Iteration 22, loss = 0.58570472\n",
      "Iteration 23, loss = 0.58527408\n",
      "Iteration 24, loss = 0.58468448\n",
      "Iteration 25, loss = 0.58351172\n",
      "Iteration 26, loss = 0.58672338\n",
      "Iteration 27, loss = 0.58408647\n",
      "Iteration 28, loss = 0.58482425\n",
      "Iteration 29, loss = 0.58411778\n",
      "Iteration 30, loss = 0.58493218\n",
      "Iteration 31, loss = 0.58523106\n",
      "Iteration 32, loss = 0.58615166\n",
      "Iteration 33, loss = 0.58719245\n",
      "Iteration 34, loss = 0.58686952\n",
      "Iteration 35, loss = 0.58506867\n",
      "Iteration 36, loss = 0.58422796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "result = [] #vetor para guardar resultados da grid search\n",
    "\n",
    "for i in range(6):\n",
    "    print(\"Executando Arquitetura: \", i+1)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes= df_arq['Neurônios'][i],\n",
    "     activation= df_arq['Função de Ativação'][i], \n",
    "     verbose= False)\n",
    "    grid = GridSearchCV(mlp, parametros)\n",
    "    grid.fit(X_train_std, Y_train)\n",
    "    result.append(grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificando a mellhor solução\n",
    "\n",
    "Como resultado da busca em grande com validação cruzada 5-fold, identifique o modelo otimizado com melhor desempenho para o problema. Apresente claramente este modelo, seus parâmetros, hiperparâmetros otimizados e resultados para cada um dos folds avaliados. Esta é a melhor solução identificada em decorrência deste projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(15, 10),\n",
       "              learning_rate_init=0.1, verbose=True)"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# Melhor\n",
    "result[0].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'batch_size': 'auto',\n",
       " 'learning_rate_init': 0.1,\n",
       " 'max_iter': 200,\n",
       " 'n_iter_no_change': 10,\n",
       " 'solver': 'adam'}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# Melhores parametros\n",
    "result[0].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melhor rede\n",
    "melhor_arquitetura = {\n",
    "    \"activation\": \"logistic\",\n",
    "    'hidden_layer_sizes':  (15, 10),\n",
    "    \"parametros\": {\n",
    "        \"batch_size\": [\"auto\"],\n",
    "        \"learning_rate_init\": [0.1],\n",
    "        \"max_iter\": [200],\n",
    "        \"n_iter_no_change\": [10],\n",
    "        \"solver\": [\"adam\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#confirmar parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=MLPClassifier(activation='logistic',\n",
       "                                     hidden_layer_sizes=(15, 10)),\n",
       "             param_grid={'batch_size': ['auto'], 'learning_rate_init': [0.1],\n",
       "                         'max_iter': [200], 'n_iter_no_change': [10],\n",
       "                         'solver': ['adam']})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 5-fold\n",
    "mlp = MLPClassifier(hidden_layer_sizes= melhor_arquitetura['hidden_layer_sizes'],\n",
    "activation= melhor_arquitetura['activation'], \n",
    "verbose= False)\n",
    "\n",
    "grid = GridSearchCV(mlp, melhor_arquitetura['parametros'], cv=5)\n",
    "grid.fit(X_train_std, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7737630805948228"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Prevendo com dados normalizados\n",
    "f1_score(Y_test, grid.predict(X_test_std), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.32742220488342205"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Prevendo com dados não normalizados\n",
    "f1_score(Y_test, grid.predict(X_test), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=MLPClassifier(activation='logistic',\n",
       "                                     hidden_layer_sizes=(15, 10)),\n",
       "             param_grid={'batch_size': ['auto'], 'learning_rate_init': [0.1],\n",
       "                         'max_iter': [200], 'n_iter_no_change': [10],\n",
       "                         'solver': ['adam']})"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# 5-fold com dados para treinamento não normalizados\n",
    "nao_normalizado = MLPClassifier(hidden_layer_sizes= melhor_arquitetura['hidden_layer_sizes'],\n",
    "activation= melhor_arquitetura['activation'], \n",
    "verbose= False)\n",
    "\n",
    "grid_nao_normalizado = GridSearchCV(nao_normalizado, melhor_arquitetura['parametros'], cv=5)\n",
    "grid_nao_normalizado.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4881012483936112"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Modelo treinado com dados não normalizados e Prevendo com dados normalizados\n",
    "f1_score(Y_test, grid_nao_normalizado.predict(X_test_std), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.4881012483936112"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Modelo treinado com dados não normalizados e Prevendo com dados não normalizados\n",
    "f1_score(Y_test, grid_nao_normalizado.predict(X_test), average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empacotando a solução\n",
    "\n",
    "Suponha que você deve entregar este classificador ao órgão responsável por administrar o Roosevelt National Park. Para tanto, você deve fazer uma preparação do mesmo para utilização neste cenário. Uma vez que já identificou os melhores parâmetros e hiperparâmetros, o passo remanescente consiste em treinar o modelo com estes valores e todos os dados disponíveis, salvando o conjunto de pesos do modelo ao final para entrega ao cliente. Assim, finalize o projeto prático realizando tais passos.\n",
    "\n",
    "1. Consulte a documentação a seguir:\n",
    "https://scikit-learn.org/stable/modules/model_persistence.html  \n",
    "2. Treine o modelo com todos os dados  \n",
    "3. Salve o modelo em disco  \n",
    "4. Construa uma rotina que recupere o modelo em disco  \n",
    "5. Mostre que a rotina é funcional, fazendo previsões com todos os elementos do dataset e exibindo uma matriz de confusão das mesmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar dados de treino dataset\n",
    "x = (x - np.mean(x))/np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=MLPClassifier(activation='logistic',\n",
       "                                     hidden_layer_sizes=(15, 10)),\n",
       "             param_grid={'batch_size': ['auto'], 'learning_rate_init': [0.1],\n",
       "                         'max_iter': [200], 'n_iter_no_change': [10],\n",
       "                         'solver': ['adam']})"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# 5-fold\n",
    "mlp = MLPClassifier(hidden_layer_sizes= melhor_arquitetura['hidden_layer_sizes'],\n",
    "activation= melhor_arquitetura['activation'], \n",
    "verbose= False)\n",
    "\n",
    "grid = GridSearchCV(mlp, melhor_arquitetura['parametros'], cv=5)\n",
    "grid.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7667173827735055"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "f1_score(y, grid.best_estimator_.predict(x), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['cover-type-model.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    " # Persistir modelo no disco\n",
    " dump(grid.best_estimator_, 'cover-type-model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar modelo no disco\n",
    "modelo_recuperado = load('cover-type-model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7667173827735055"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "# Verificando se o modelo está funcional\n",
    "f1_score(y, modelo_recuperado.predict(x), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}